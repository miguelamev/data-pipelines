{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguelamev/data-pipelines/blob/main/ClickUp_Data_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pRF4FhDcc--"
      },
      "source": [
        "# Botrista ETL Pipeline\n",
        "\n",
        "The goal of this project is to automate the ETL process of Botrista's sales data sources for easy analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "B7pFLdTZnX0B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.auth.transport.requests import Request\n",
        "from google.auth import default\n",
        "import gspread\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "\n",
        "import time\n",
        "import progressbar\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN6o1OnIedW1",
        "outputId": "50dfb552-4bb3-4b66-bc1a-0d67663a0ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "pd.set_option('display.max_rows', None) # Show all rows\n",
        "pd.set_option('display.max_columns', None) # Show all columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Area index\n",
        "area_path = 'drive/MyDrive/02 Areas/Business Intelligence/Indexes/Area Index.csv'\n",
        "area_index = pd.read_csv(area_path)\n",
        "\n",
        "# Brand index\n",
        "brand_path = 'drive/MyDrive/02 Areas/Business Intelligence/Indexes/Brand Index.csv'\n",
        "brand_index = pd.read_csv(brand_path)"
      ],
      "metadata": {
        "id": "C2P-30UdkNxq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPzCKVM5YxXw"
      },
      "source": [
        "## ClickUp Tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the .env file from Google Drive (adjust the path)\n",
        "load_dotenv(\"/content/drive/MyDrive/02 Areas/Business Intelligence/Keys/clickup_key.env\")  # Adjust the path if needed\n",
        "\n",
        "# Fetch the API key from the .env file\n",
        "API_TOKEN = os.getenv(\"CLICKUP_API_KEY\")\n",
        "\n",
        "# Check if the API key was successfully loaded\n",
        "if API_TOKEN:\n",
        "    print(\"Successfully loaded API Key\")\n",
        "else:\n",
        "    print(\"Error: API key not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZSseg7m67vn",
        "outputId": "1e3b9135-9cbd-4164-eaa6-cf6f808b682b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded API Key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Vi2hLKO1atAT"
      },
      "outputs": [],
      "source": [
        "HEADERS = {\n",
        "    \"Authorization\": API_TOKEN,\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# === Key ID Definitions === #\n",
        "\n",
        "## TeamID\n",
        "TEAM_ID = \"12612276\"\n",
        "\n",
        "## SpaceIDs\n",
        "Deployments_SPACEID = \"18930510\"\n",
        "\n",
        "## FolderIDs\n",
        "InstallBase_FOLDERID = \"108028980\"\n",
        "\n",
        "## ListIDs\n",
        "LiveMachines_LISTID = \"170916222\"\n",
        "DecomSwap_LISTID = \"164425124\"\n",
        "CurrentDeployments_LISTID = \"180119627\"\n",
        "PlumbingJob_LISTID = \"901400603529\"\n",
        "PostDeployments_LISTID = \"901408037105\"\n",
        "\n",
        "## TaskIDs\n",
        "test_TASKID = \"86b328wdy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-9dcHJUci-M"
      },
      "source": [
        "## Extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Y-oHRjDLGEjO"
      },
      "outputs": [],
      "source": [
        "# GET Function Definitions:\n",
        "\n",
        "## Function to get team details\n",
        "def get_teams():\n",
        "    url = \"https://api.clickup.com/api/v2/team\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()  # Returns full task details\n",
        "    else:\n",
        "        return {\"error\": f\"Failed to fetch team: {response.status_code}\", \"details\": response.text}\n",
        "\n",
        "## Function to get folder information\n",
        "def get_folder(folder_id):\n",
        "    url = f\"https://api.clickup.com/api/v2/folder/{folder_id}/list?archived=false\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "    if response.status_code == 200:\n",
        "\n",
        "        return response.json()  # Returns full task details\n",
        "    else:\n",
        "        return {\"error\": f\"Failed to fetch folder: {response.status_code}\", \"details\": response.text}\n",
        "\n",
        "## Function to get list information\n",
        "def get_list(list_id):\n",
        "    url = f\"https://api.clickup.com/api/v2/list/{list_id}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()  # Returns full task details\n",
        "    else:\n",
        "        return {\"error\": f\"Failed to fetch list: {response.status_code}\", \"details\": response.text}\n",
        "\n",
        "## Function to get the custom fields allowed on a list:\n",
        "def get_custom_fields_from_list(list_id):\n",
        "  url = f\"https://api.clickup.com/api/v2/list/{list_id}/field\"\n",
        "  response = requests.get(url, headers=HEADERS)\n",
        "  if response.status_code == 200:\n",
        "        return response.json()  # Returns full task details\n",
        "  else:\n",
        "        return {\"error\": f\"Failed to fetch custom fields: {response.status_code}\", \"details\": response.text}\n",
        "\n",
        "## Function to get all tasks from a list with a progress bar\n",
        "def get_all_tasks(list_id):\n",
        "    all_tasks = []\n",
        "    page = 0  # Start from the first page\n",
        "\n",
        "    with tqdm(desc=\"Fetching Tasks\", unit=\"page\") as pbar:\n",
        "        while True:\n",
        "            url = f\"https://api.clickup.com/api/v2/list/{list_id}/task?page={page}\"\n",
        "            response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error: {response.status_code}, {response.text}\")\n",
        "                break  # Stop if there's an error\n",
        "\n",
        "            data = response.json()\n",
        "            tasks = data.get(\"tasks\", [])\n",
        "\n",
        "            if not tasks:\n",
        "                break  # Stop when no more tasks are found\n",
        "\n",
        "            all_tasks.extend(tasks)  # Add tasks from this page\n",
        "            page += 1  # Move to the next page\n",
        "            pbar.update(1)  # Increment progress bar for each page fetched\n",
        "\n",
        "    return all_tasks\n",
        "\n",
        "## Function to get task details\n",
        "def get_task_details(task_id):\n",
        "    url = f\"https://api.clickup.com/api/v2/task/{task_id}\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()  # Returns full task details\n",
        "    else:\n",
        "        return {\"error\": f\"Failed to fetch task: {response.status_code}\", \"details\": response.text}\n",
        "\n",
        "## Function to get tags available in a space\n",
        "def get_tags_from_space(space_id):\n",
        "    url = f\"https://api.clickup.com/api/v2/space/{space_id}/tag\"\n",
        "    response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        return response.json()  # Returns full task details\n",
        "    else:\n",
        "        return {\"error\": f\"Failed to fetch task: {response.status_code}\", \"details\": response.text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GSIJYH4JyGOt"
      },
      "outputs": [],
      "source": [
        "# EXTRACTION Function Definition:\n",
        "\n",
        "## Extracting TaskIDs from raw list data object.\n",
        "def extract_task_ids(tasks):\n",
        "    \"\"\"Extracts all task IDs from a list of tasks.\"\"\"\n",
        "    return [task[\"id\"] for task in tasks if \"id\" in task]\n",
        "\n",
        "def extract_custom_fields(field_list):\n",
        "    \"\"\"Extracts all custom fields allowed in a list.\"\"\"\n",
        "\n",
        "    fields = []  # Initialize an empty list to store extracted field data\n",
        "    for field in field_list['fields']:\n",
        "        fields.append({\n",
        "            'Name': field.get('name'),\n",
        "            'Type': field.get('type'),\n",
        "            'Date_created': field.get('date_created'),\n",
        "            'Required': field.get('required')\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(fields)  # Convert list of dictionaries to a DataFrame\n",
        "\n",
        "## Extracting task details with a progress bar\n",
        "def extract_task_details(task_ids):\n",
        "    all_tasks_data = []  # List to store details of all tasks\n",
        "\n",
        "    with tqdm(total=len(task_ids), desc=\"Processing Tasks\", unit=\"task\") as pbar:\n",
        "        for task_id in task_ids:\n",
        "            taskdetails = get_task_details(task_id)  # Fetch task details\n",
        "\n",
        "            task_data = {\n",
        "                \"Task ID\": task_id,\n",
        "                \"Task Name\": taskdetails.get('name'),\n",
        "                \"Status\": taskdetails.get('status', {}).get('status'),\n",
        "                \"ClickUp URL\": taskdetails.get('URL'),\n",
        "                \"Tags\": [tag.get('name') for tag in taskdetails.get('tags',{})], #Extracting tags.\n",
        "            }\n",
        "\n",
        "            # Extract custom fields and add them as individual columns\n",
        "            for field in taskdetails.get('custom_fields', []):\n",
        "                key = field.get('name')  # Get field name\n",
        "                value = field.get('value')  # Get field value\n",
        "\n",
        "                if key and key.strip():  # Ensure key is valid\n",
        "                    task_data[key] = value  # Add custom field as a new column\n",
        "\n",
        "            # Add metadata at the end\n",
        "            task_data[\"List ID\"] = taskdetails.get('list').get('id')\n",
        "            task_data[\"Project ID\"] = taskdetails.get('project').get('id')\n",
        "            task_data[\"Folder ID\"] = taskdetails.get('folder').get('id')\n",
        "            task_data[\"Space ID\"] = taskdetails.get('space').get('id')\n",
        "\n",
        "            all_tasks_data.append(task_data)  # Append task details to list\n",
        "            pbar.update(1)  # Update the progress bar for each task processed\n",
        "\n",
        "    return all_tasks_data  # Return structured data\n",
        "\n",
        "## Extracts tag names from raw tag information.\n",
        "def extract_tag_names_from_space(space_id):\n",
        "    \"\"\"\n",
        "    Extracts tag names from a given tags dictionary.\n",
        "\n",
        "    Parameters:\n",
        "        tags_raw (dict): The dictionary containing tag information.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tag names to be used as column names.\n",
        "    \"\"\"\n",
        "    tags_raw = get_tags_from_space(space_id)\n",
        "    tag_names = [tag['name'] for tag in tags_raw.get('tags',{})]\n",
        "    return tag_names\n",
        "\n",
        "## Identifies and flattens out any fields that have nested dictionaries or lists of dictionries.\n",
        "def flatten_nested_data(data_list):\n",
        "    \"\"\"Flattens nested dictionaries and lists in a list of dictionaries.\n",
        "       - Expands dictionaries into individual keys.\n",
        "       - Extracts lists of single values or dictionaries into direct values.\n",
        "    \"\"\"\n",
        "    flattened_data = []\n",
        "\n",
        "    for entry in data_list:  # Iterate over each dictionary in the list\n",
        "        if not isinstance(entry, dict):\n",
        "            continue  # Skip non-dictionary items\n",
        "\n",
        "        flat_entry = {}  # Store flattened key-value pairs\n",
        "\n",
        "        def flatten_value(key, value, parent_key=None):\n",
        "            \"\"\" Helper function to handle flattening of nested dictionaries and lists \"\"\"\n",
        "            if isinstance(value, dict):  # If it's a dictionary\n",
        "                for sub_key, sub_value in value.items():\n",
        "                    flatten_value(sub_key, sub_value, parent_key=f\"{parent_key}_{key}\" if parent_key else key)\n",
        "            elif isinstance(value, list):  # If it's a list\n",
        "                if all(isinstance(item, dict) for item in value):  # List of dictionaries\n",
        "                    for sub_key in set().union(*(d.keys() for d in value)):  # Get all keys in the list of dicts\n",
        "                        extracted_values = [d.get(sub_key) for d in value if d.get(sub_key) is not None]\n",
        "                        # If there's only one value, extract it directly; otherwise, keep the list\n",
        "                        flat_entry[f\"{key}_{sub_key}\"] = extracted_values[0] if len(extracted_values) == 1 else extracted_values\n",
        "                else:  # List of values (non-dictionaries)\n",
        "                    flat_entry[key] = value[0] if len(value) == 1 else \", \".join(map(str, value))\n",
        "            else:\n",
        "                # For simple values, just store it\n",
        "                flat_entry[key] = value\n",
        "\n",
        "        for key, value in entry.items():\n",
        "            flatten_value(key, value)  # Start flattening for each key-value pair\n",
        "\n",
        "        flattened_data.append(flat_entry)  # Add to the result list\n",
        "\n",
        "    return flattened_data  # Returns a list of dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs5nJR2fcmVB"
      },
      "source": [
        "## Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kebx7Z2uNA8i"
      },
      "outputs": [],
      "source": [
        "# TRANSFORMATION : Dealing with Null Values and Setting column names\n",
        "\n",
        "def analyze_columns(df):\n",
        "    \"\"\"Analyzes a DataFrame by sorting columns alphabetically,\n",
        "       checking data types, unique values, and null counts.\n",
        "    \"\"\"\n",
        "    # Sort columns alphabetically\n",
        "    sorted_columns = sorted(df.columns)\n",
        "\n",
        "    analysis = []\n",
        "\n",
        "    for col in sorted_columns:\n",
        "        col_data = df[col]  # Extract column data\n",
        "\n",
        "        # Handle unhashable types (lists, dictionaries)\n",
        "        converted_data = col_data.apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
        "\n",
        "        # Unique count (handling lists safely)\n",
        "        try:\n",
        "            unique_count = converted_data.nunique()\n",
        "        except TypeError:\n",
        "            unique_count = len(set(map(str, col_data.dropna())))  # Convert lists to strings for uniqueness\n",
        "\n",
        "        analysis.append({\n",
        "            \"Column Name\": col,\n",
        "            \"Data Type\": col_data.dtype,\n",
        "            \"Unique Values\": unique_count,\n",
        "            \"Total Values\": len(col_data),\n",
        "            \"Null Count\": col_data.isna().sum(),\n",
        "            \"Null Percentage\": round((col_data.isna().sum() / len(col_data)) * 100,2)\n",
        "        })\n",
        "    return pd.DataFrame(analysis)  # Return results as a DataFrame\n",
        "\n",
        "\n",
        "## Removes Nan filled columns and rows\n",
        "def lean_dataframe(df, min_fill_col_percentage, min_fill_row_percentage):\n",
        "    \"\"\"\n",
        "    Filters a DataFrame by keeping only:\n",
        "    - Columns with at least `min_fill_col_percentage` of non-null values.\n",
        "    - Rows with at least `min_fill_row_percentage` of non-null values.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input dataset.\n",
        "        min_fill_col_percentage (float): Minimum percentage (0-100) of non-null values for columns.\n",
        "        min_fill_row_percentage (float): Minimum percentage (0-100) of non-null values for rows.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleaned DataFrame with only the valid columns and rows.\n",
        "    \"\"\"\n",
        "    # Keep columns with sufficient non-null values\n",
        "    df = df.loc[:, df.notnull().mean() * 100 >= min_fill_col_percentage]\n",
        "\n",
        "    # Keep rows with sufficient non-null values\n",
        "    return df.loc[df.notnull().mean(axis=1) * 100 >= min_fill_row_percentage]\n",
        "\n",
        "## One-hot encodes tag columns to yield True and False values.\n",
        "def expand_tags_column(df, tag_column):\n",
        "    \"\"\"\n",
        "    Expands a column containing comma-separated tags into individual binary columns with formatted names.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        tag_column (str): The column name containing the tags.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The transformed DataFrame with separate binary columns for each tag.\n",
        "    \"\"\"\n",
        "    # Fill NaNs with empty strings to avoid errors\n",
        "    df[tag_column] = df[tag_column].fillna(\"\")\n",
        "\n",
        "    # Split tags into lists\n",
        "    df[\"Tag_List\"] = df[tag_column].apply(lambda x: x.split(\", \") if x else [])\n",
        "\n",
        "    # Get unique tags from all rows\n",
        "    unique_tags = set(tag for tags in df[\"Tag_List\"] for tag in tags)\n",
        "\n",
        "    # Create new columns with the format [Tag name] (Tag)\n",
        "    for tag in unique_tags:\n",
        "        formatted_col_name = f\"[{tag}] Tag\"\n",
        "        df[formatted_col_name] = df[\"Tag_List\"].apply(lambda tags: tag in tags)\n",
        "\n",
        "    # Drop the temporary list column\n",
        "    df.drop(columns=[\"Tag_List\"], inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q1dnQ6xZ83SI"
      },
      "outputs": [],
      "source": [
        "# TRANSFORMATION : Data Standardization\n",
        "\n",
        "def validate_email(email):\n",
        "    \"\"\"Check if the string is a valid email.\"\"\"\n",
        "    email_regex = r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$\"\n",
        "    return bool(re.match(email_regex, str(email)))\n",
        "\n",
        "def validate_phone(phone):\n",
        "    \"\"\"Check if the string is a valid phone number (basic validation).\"\"\"\n",
        "    phone_regex = r\"^\\+?[0-9().\\-\\s]{7,15}$\"\n",
        "    return bool(re.match(phone_regex, str(phone)))\n",
        "\n",
        "def is_unix_timestamp(value):\n",
        "    \"\"\"Check if a value is a Unix timestamp (either in seconds or milliseconds).\"\"\"\n",
        "    if isinstance(value, (int, float)) and not np.isnan(value):\n",
        "        value = int(value)  # Ensure value is an integer before checking\n",
        "        if 1000000000 <= value <= 4102444800:  # Approx. 2001 to 2100 (seconds)\n",
        "            return \"seconds\"\n",
        "        elif 1000000000000 <= value <= 4102444800000:  # Approx. 2001 to 2100 (milliseconds)\n",
        "            return \"milliseconds\"\n",
        "    return False\n",
        "\n",
        "def standardize_column_types(dataset, column_names=None, column_types=None):\n",
        "    \"\"\"\n",
        "    Standardizes and/or converts the columns in `dataset` based on the rules defined in `column_names` and `column_types`.\n",
        "    If `column_names` and `column_types` are not provided, it standardizes column types across the dataset.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (pd.DataFrame): The dataset to standardize/convert.\n",
        "        column_names (list or pd.Series, optional): The list of column names to convert. Defaults to None.\n",
        "        column_types (list or pd.Series, optional): The corresponding data types for conversion. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The modified dataset with updated and standardized data types.\n",
        "    \"\"\"\n",
        "    if column_names is not None and column_types is not None:\n",
        "        for col_name, col_type in zip(column_names, column_types):\n",
        "            if col_name not in dataset.columns:\n",
        "                continue  # Skip columns that are not in the dataset\n",
        "\n",
        "            # Convert potential date columns to numeric first\n",
        "            if col_type in ['date']:\n",
        "                dataset[col_name] = pd.to_numeric(dataset[col_name], errors='coerce')\n",
        "\n",
        "            if col_type in ['short_text', 'text', 'labels', 'users', 'tasks', 'formula']:\n",
        "                dataset[col_name] = dataset[col_name].astype(str)\n",
        "\n",
        "            elif col_type == 'drop_down':\n",
        "                dataset[col_name] = dataset[col_name].apply(lambda x: str(int(float(x))) if pd.notna(x) and x not in [\"\", None] else \"\")\n",
        "\n",
        "            elif col_type == 'url':\n",
        "                dataset[col_name] = dataset[col_name].apply(lambda x: x if urlparse(str(x)).scheme else np.nan)\n",
        "\n",
        "            elif col_type == 'email':\n",
        "                dataset[col_name] = dataset[col_name].apply(lambda x: x if validate_email(x) else np.nan)\n",
        "\n",
        "            elif col_type == 'checkbox':\n",
        "                dataset[col_name] = dataset[col_name].astype(bool)\n",
        "\n",
        "            elif col_type == 'phone':\n",
        "                dataset[col_name] = dataset[col_name].apply(lambda x: x if validate_phone(x) else np.nan)\n",
        "\n",
        "            elif col_type in ['currency', 'number']:\n",
        "                dataset[col_name] = pd.to_numeric(dataset[col_name], errors='coerce')\n",
        "\n",
        "            elif col_type == 'date':\n",
        "                dataset[col_name] = dataset[col_name].apply(\n",
        "                    lambda x: pd.to_datetime(int(x), unit='s').date() if is_unix_timestamp(x) == \"seconds\"\n",
        "                    else pd.to_datetime(int(x), unit='ms').date() if is_unix_timestamp(x) == \"milliseconds\"\n",
        "                    else np.nan\n",
        "                )\n",
        "\n",
        "    else:\n",
        "        # Standardize column types across the dataset\n",
        "        for col in dataset.columns:\n",
        "            # Drop completely empty columns\n",
        "            if dataset[col].isna().all():\n",
        "                continue\n",
        "\n",
        "            # Convert object-type columns to numeric if possible\n",
        "            if dataset[col].dtype == \"object\":\n",
        "                dataset[col] = pd.to_numeric(dataset[col], errors='ignore')\n",
        "\n",
        "            # Infer the dominant data type in the column\n",
        "            inferred_type = dataset[col].dropna().map(type).mode()[0] if not dataset[col].dropna().empty else str\n",
        "\n",
        "            # Standardize None and NaN values\n",
        "            dataset[col] = dataset[col].replace({None: np.nan})\n",
        "\n",
        "            # Apply appropriate conversion based on inferred type\n",
        "            if inferred_type == str:\n",
        "                dataset[col] = dataset[col].astype(str).replace('nan', np.nan)  # Ensure empty strings convert to NaN\n",
        "            elif inferred_type in [int, float, np.number]:\n",
        "                dataset[col] = pd.to_numeric(dataset[col], errors='coerce')\n",
        "\n",
        "                # Check if numeric column is a Unix timestamp\n",
        "                if dataset[col].apply(is_unix_timestamp).any():\n",
        "                    dataset[col] = dataset[col].apply(\n",
        "                        lambda x: pd.to_datetime(int(x), unit='s').date() if is_unix_timestamp(x) == \"seconds\"\n",
        "                        else pd.to_datetime(int(x), unit='ms').date() if is_unix_timestamp(x) == \"milliseconds\"\n",
        "                        else np.nan\n",
        "                    )\n",
        "\n",
        "            elif inferred_type == bool:\n",
        "                dataset[col] = dataset[col].astype(bool)\n",
        "            elif inferred_type == pd.Timestamp:\n",
        "                dataset[col] = pd.to_datetime(dataset[col], errors='coerce')\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hD5XBlq_KZuc"
      },
      "outputs": [],
      "source": [
        "# TRANSFORMATION : Dealing with indexed columns.\n",
        "\n",
        "def get_coltype_columns(column_names, column_types, coltype):\n",
        "    \"\"\"\n",
        "    Identifies and returns a list of column names that are of type 'drop_down'.\n",
        "\n",
        "    Parameters:\n",
        "        column_names (list or pd.Series): The list of column names.\n",
        "        column_types (list or pd.Series): The corresponding data types.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of column names that are dropdowns.\n",
        "    \"\"\"\n",
        "    return [col_name for col_name, col_type in zip(column_names, column_types) if col_type == coltype]\n",
        "\n",
        "## Build a dropdown from the raw data from Custom Fields.\n",
        "def build_lookup(fields):\n",
        "    \"\"\"\n",
        "    Extracts dropdown field mappings from a list of fields.\n",
        "\n",
        "    Parameters:\n",
        "        fields (dict): Dictionary containing field metadata.\n",
        "        E.g. get_custom_fields_from_list(list_id)\n",
        "\n",
        "    Returns:\n",
        "        dict: A lookup dictionary where keys are field names,\n",
        "              and values are dictionaries mapping indices to labels.\n",
        "    \"\"\"\n",
        "    lookup_table = {}\n",
        "\n",
        "    for field in fields.get('fields', []):  # Safely get 'fields' list\n",
        "        if field.get('type') == 'drop_down':  # Check if field is dropdown\n",
        "            lookup_table[field['name']] = {\n",
        "                str(option.get('orderindex')): str(option.get('name'))  # Use 'NA' or 'Unknown' if key doesn't exist\n",
        "                for option in field.get('type_config', {}).get('options', [])\n",
        "            }\n",
        "\n",
        "        if field.get('type') == 'labels':  # Check if field is labels\n",
        "            lookup_table[field['name']] = {\n",
        "                str(option.get('id')): str(option.get('label'))  # Use 'NA' or 'Unknown' if key doesn't exist\n",
        "                for option in field.get('type_config', {}).get('options', [])\n",
        "            }\n",
        "\n",
        "    return lookup_table\n",
        "\n",
        "## Use LookUp Table to convert indexes into values\n",
        "def replace_indexes(dataset, lookup_table):\n",
        "    \"\"\"\n",
        "    Replaces indexed dropdown values with their corresponding labels using the lookup table.\n",
        "    Handles both single and multiple-choice fields.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (pd.DataFrame): The dataset where dropdown columns have indexed values.\n",
        "        lookup_table (dict): A dictionary where keys are dropdown column names,\n",
        "                             and values are dictionaries mapping index -> label.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A modified dataset with dropdown indexes replaced by their corresponding labels.\n",
        "    \"\"\"\n",
        "    df = dataset.copy()  # Avoid modifying the original dataset\n",
        "\n",
        "    for col, mapping in lookup_table.items():\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(str).apply(\n",
        "                lambda x: \", \".join([mapping.get(val, val) for val in x.split(\", \")]) if pd.notna(x) else x\n",
        "            )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh8CqQ-A-5Ys"
      },
      "source": [
        "## Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Obd8lVTQDb8Y"
      },
      "outputs": [],
      "source": [
        "# Defining Cleaning functions\n",
        "\n",
        "## STATUS\n",
        "def process_status_column(df, column_name=\"Status\"):\n",
        "    def clean_status(status):\n",
        "        # Convert to Title Case\n",
        "        title_case_status = status.title()\n",
        "\n",
        "        return title_case_status\n",
        "\n",
        "    df[column_name] = df[column_name].apply(clean_status)\n",
        "    return df\n",
        "\n",
        "## CUSTOMER TYPE\n",
        "def process_customer_type(df, column_name=\"Customer Type\"):\n",
        "    def categorize_customer_type(customer_type):\n",
        "        # Ensure it's a string and handle NaNs\n",
        "        if pd.isna(customer_type):\n",
        "            return pd.Series([\"Other\", \"NA\"])  # Default values for NaNs\n",
        "\n",
        "        customer_type = str(customer_type).title()  # Convert to string and standardize casing\n",
        "\n",
        "        # Determine category\n",
        "        if \"Channel\" in customer_type:\n",
        "            category = \"Channel\"\n",
        "        elif \"Enterprise\" in customer_type:\n",
        "            category = \"Enterprise\"\n",
        "        elif \"Smb\" in customer_type:  # Normalize \"SMB\"\n",
        "            category = \"Mid-Market\"\n",
        "        else:\n",
        "            category = \"Other\"\n",
        "\n",
        "        return pd.Series([category])\n",
        "\n",
        "    df[[\"Customer Type\"]] = df[column_name].apply(categorize_customer_type)\n",
        "    return df\n",
        "\n",
        "## VERTICAL\n",
        "def process_vertical_column(df, column_name=\"Vertical\"):\n",
        "    valid_values = {\n",
        "        \"Education\", \"QSR\", \"SMB\", \"Theme Parks\", \"Cinema\",\n",
        "        \"Healthcare & Seniors\", \"Sports & Leisure\", \"Business & Industry\"\n",
        "    }\n",
        "\n",
        "    smb_aliases = {\n",
        "        \"High Traffic Chain/Business\",\n",
        "        \"High Potential Growth Business\",\n",
        "        \"Independent Business\"\n",
        "    }\n",
        "\n",
        "    def clean_vertical(value):\n",
        "        if value in smb_aliases:\n",
        "            return \"SMB\"\n",
        "        return value if value in valid_values else value\n",
        "\n",
        "    df[column_name] = df[column_name].apply(clean_vertical)\n",
        "    return df\n",
        "\n",
        "## BRAND\n",
        "def process_brand_column(df, column_name=\"Brand\"):\n",
        "    def clean_brand(value):\n",
        "        if pd.isna(value):  # Handle NaN values first\n",
        "            return \"Other\"\n",
        "\n",
        "        value = str(value).strip()  # Convert to string and remove extra spaces\n",
        "\n",
        "        # Remove \"SMB -\" and \"Channel -\"\n",
        "        cleaned_value = re.sub(r\"^(SMB|Channel) - \", \"\", value)\n",
        "\n",
        "        return cleaned_value\n",
        "\n",
        "    df[column_name] = df[column_name].apply(clean_brand)\n",
        "    return df\n",
        "\n",
        "## MACHINE DELIVERY DATE\n",
        "def fill_machine_delivery_date(df):\n",
        "    # Define the possible columns in order of priority\n",
        "    columns = [\n",
        "        \"Machine delivery Date\",\n",
        "        \"Bot Delivery Date\",\n",
        "        \"Install Date\",\n",
        "        \"Install Date (duplicated)\"\n",
        "    ]\n",
        "\n",
        "    # Filter only existing columns\n",
        "    available_cols = [col for col in columns if col in df.columns]\n",
        "\n",
        "    if len(available_cols) > 1:\n",
        "        # Use ffill over the available columns\n",
        "        df[\"Machine delivery Date\"] = df[available_cols].bfill(axis=1).iloc[:, 0]\n",
        "\n",
        "    # Ensure NaT remains if all possible columns were empty\n",
        "    df[\"Machine delivery Date\"] = df.apply(\n",
        "        lambda row: pd.NaT if row[available_cols].isna().all() else row[\"Machine delivery Date\"],\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "## SITE WORK\n",
        "def format_site_work(df, column_name=\"site work\"):\n",
        "    mapping = {\n",
        "        \"done\": \"Finished\",\n",
        "        \"not required\": \"Not Required\",\n",
        "        \"Botrista in progress\": \"In Progress by Botrista\",\n",
        "        \"Customer in progress\": \"In Progress by Customer\",\n",
        "        \"Ninja\": \"In Progress by Botrista\"  # Assuming \"Ninja\" means Botrista is handling it\n",
        "    }\n",
        "\n",
        "    df[column_name] = df[column_name].map(mapping).fillna(df[column_name])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qFA2IahO82Z2"
      },
      "outputs": [],
      "source": [
        "# Filling in empty VERTICAL, CUSTOMER TYPE, BRAND:\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_index_table(filled_df):\n",
        "    \"\"\"\n",
        "    Create an index table using 'Brand' as the unique key, mapping to Vertical and Customer Type.\n",
        "\n",
        "    Parameters:\n",
        "        filled_df (pd.DataFrame): DataFrame with non-empty 'Vertical', 'Customer Type', and 'Brand' columns.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping each Brand to its most common (Vertical, Customer Type).\n",
        "    \"\"\"\n",
        "    # Group by Brand and select the most frequent Vertical and Customer Type\n",
        "    index_table = (\n",
        "        filled_df.groupby('Brand')[['Vertical', 'Customer Type']]\n",
        "        .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "        .to_dict(orient='index')\n",
        "    )\n",
        "    return index_table  # Returns {Brand: {'Vertical': X, 'Customer Type': Y}}\n",
        "\n",
        "def find_brand_from_location(location_name, brands):\n",
        "    \"\"\"\n",
        "    Try to find the best-matching brand from the 'Location Name' using substring search.\n",
        "\n",
        "    Parameters:\n",
        "        location_name (str): The name of the location.\n",
        "        brands (list): List of known brand names.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The best-matching brand or None if no match is found.\n",
        "    \"\"\"\n",
        "    if pd.isna(location_name):\n",
        "        return None\n",
        "    location_name_lower = location_name.lower()\n",
        "\n",
        "    for brand in brands:\n",
        "        if brand.lower() in location_name_lower:\n",
        "            return brand\n",
        "    return None\n",
        "\n",
        "def autocomplete_table(incomplete_df, index_table):\n",
        "    \"\"\"\n",
        "    Autocomplete missing 'Vertical', 'Customer Type', and 'Brand' in an incomplete dataset.\n",
        "\n",
        "    Parameters:\n",
        "        incomplete_df (pd.DataFrame): DataFrame with missing values in 'Vertical', 'Customer Type', and 'Brand'.\n",
        "        index_table (dict): Dictionary of Brand -> {'Vertical': X, 'Customer Type': Y}\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The completed dataset with missing values filled.\n",
        "    \"\"\"\n",
        "    brands = list(index_table.keys())\n",
        "\n",
        "    for i, row in incomplete_df.iterrows():\n",
        "        if pd.notna(row['Brand']) and row['Brand'] in index_table:\n",
        "            # Fill missing Vertical and Customer Type using index table\n",
        "            incomplete_df.at[i, 'Vertical'] = index_table[row['Brand']]['Vertical']\n",
        "            incomplete_df.at[i, 'Customer Type'] = index_table[row['Brand']]['Customer Type']\n",
        "        elif pd.isna(row['Brand']):\n",
        "            # Try finding the Brand from Location Name\n",
        "            detected_brand = find_brand_from_location(row['Location Name'], brands)\n",
        "            if detected_brand:\n",
        "                incomplete_df.at[i, 'Brand'] = detected_brand\n",
        "                incomplete_df.at[i, 'Vertical'] = index_table[detected_brand]['Vertical']\n",
        "                incomplete_df.at[i, 'Customer Type'] = index_table[detected_brand]['Customer Type']\n",
        "\n",
        "    return incomplete_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vt14VycZ7qZv"
      },
      "outputs": [],
      "source": [
        "def format_title_case_in_dataframe(df, columns):\n",
        "    # Function to format individual string values or list of strings to title case with no extra spaces\n",
        "    def format_title_case(input_value):\n",
        "        # Handle cases where the value is NaN or None\n",
        "        if pd.isna(input_value) or input_value is None:\n",
        "            return None\n",
        "\n",
        "        # If it's a list, process each element in the list\n",
        "        if isinstance(input_value, list):\n",
        "            return ' '.join([format_title_case(item) for item in input_value])\n",
        "\n",
        "        # Ensure input is a string and handle it\n",
        "        if isinstance(input_value, str):\n",
        "            # Remove extra spaces and convert to title case\n",
        "            formatted_string = ' '.join(input_value.split())\n",
        "            return formatted_string.title()\n",
        "\n",
        "        # If it's neither a string nor a list, return it as is\n",
        "        return input_value\n",
        "\n",
        "    # Apply the formatting to the specified columns\n",
        "    for column in columns:\n",
        "        if column in df.columns:\n",
        "            df[column] = df[column].apply(lambda x: format_title_case(x) if isinstance(x, (str, list, type(None))) else x)\n",
        "        else:\n",
        "            raise ValueError(f\"Column '{column}' not found in the DataFrame.\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Index by Brand ###\n",
        "def fill_vertical_by_brand(df, brand_index):\n",
        "    \"\"\"\n",
        "    Fills missing values in LiveDecom['Vertical'] based on the first three characters of LiveDecom['Location ID']\n",
        "    using brand_index.\n",
        "\n",
        "    Parameters:\n",
        "    livedecom_df (pd.DataFrame): DataFrame containing 'Location ID' and 'Vertical' columns.\n",
        "    brand_index_df (pd.DataFrame): DataFrame containing 'Brand ID' and 'Vertical' columns.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Updated DataFrame with missing or empty 'Vertical' values filled.\n",
        "    \"\"\"\n",
        "    # Create a mapping dictionary from brand_index\n",
        "    brand_map = brand_index.set_index('Brand ID')['Vertical'].to_dict()\n",
        "\n",
        "    # Fill missing or empty values based on the first three characters of 'Location ID'\n",
        "    df['Vertical'] = df.apply(\n",
        "        lambda row: brand_map.get(row['Location ID'][:3], row['Vertical'])\n",
        "        if (pd.isna(row['Vertical']) or row['Vertical'] == '') else row['Vertical'], axis=1\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def fill_region_by_state(df, area_index):\n",
        "    \"\"\"\n",
        "    Adds a 'Region' column to the DataFrame based on 'State', using the 'Abbreviation' in area_index.\n",
        "    If 'State' is null, empty, or not found, 'Region' will be left as NaN.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): DataFrame containing a 'State' column.\n",
        "    area_index (pd.DataFrame): DataFrame containing 'Abbreviation' and 'Region' columns.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Updated DataFrame with new 'Region' column.\n",
        "    \"\"\"\n",
        "    # Create a mapping dictionary from area_index using 'Abbreviation'\n",
        "    area_map = area_index.set_index('Abbreviation')['Region'].to_dict()\n",
        "\n",
        "    # Assign region directly based on state abbreviation\n",
        "    df['Region'] = df['State'].map(area_map)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "dT2VW4T604c7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Tr3zoOAw-7mN"
      },
      "outputs": [],
      "source": [
        "# CLEAN : Live Machines\n",
        "\n",
        "def clean_LiveMachines(df, colnames):\n",
        "  df = process_status_column(df)\n",
        "  df = process_customer_type(df)\n",
        "  df = process_vertical_column(df)\n",
        "  df = process_brand_column(df)\n",
        "  df = fill_machine_delivery_date(df)\n",
        "  df = format_site_work(df)\n",
        "  df = fill_vertical_by_brand(df, brand_index)\n",
        "  df = fill_region_by_state(df, area_index)\n",
        "\n",
        "  # Appending tag columns\n",
        "  tag_columns = [column for column in df.columns if \"Tag\" in column]\n",
        "\n",
        "  # Renaming and Reordering\n",
        "  df = df.rename(columns=colnames)  # Rename columns\n",
        "  df = df[list(list(colnames.values()) + tag_columns)]  # Reorder columns\n",
        "\n",
        "  return df\n",
        "\n",
        "LM_colnames = {\n",
        "    \"Location ID\": \"Location ID\",\n",
        "    \"Task Name\": \"Location Name\",\n",
        "    \"Vertical\": \"Vertical\",\n",
        "    \"Customer Type\": \"Customer Type\",\n",
        "    \"Brand\": \"Brand\",\n",
        "    \"Launch Date\": \"Launch Date\",\n",
        "    \"Install Date\": \"Install Date\",\n",
        "    \"Machine delivery Date\": \"Machine Delivery Date\",\n",
        "    \"Survey Date\": \"Survey Date\",\n",
        "    \"Region\": \"Region\",\n",
        "    \"State\": \"State\",\n",
        "    \"formatted_address\": \"Formatted Address\",\n",
        "    \"lat\": \"Latitude\",\n",
        "    \"lng\": \"Longitude\",\n",
        "    \"place_id\": \"Place ID\",\n",
        "    \"Operator\": \"Operator\",\n",
        "    \"Contract Type\": \"Contract Type\",\n",
        "    \"site work\": \"Site Work\",\n",
        "    \"Site GM Name\": \"Site GM Name\",\n",
        "    \"Site GM number\": \"Site GM Number\",\n",
        "    \"Site GM Email\": \"Site GM Email\",\n",
        "    \"Sales_id\": \"Sales ID\",\n",
        "    \"Sales_username\": \"Sales Name\",\n",
        "    \"Sales_email\": \"Sales Email\",\n",
        "    \"RAE_id\": \"RAE ID\",\n",
        "    \"RAE_username\": \"RAE Name\",\n",
        "    \"RAE_email\": \"RAE Email\",\n",
        "    \"SOE_id\": \"SOE ID\",\n",
        "    \"SOE_username\": \"SOE Name\",\n",
        "    \"SOE_email\": \"SOE Email\",\n",
        "    \"CC Agent_id\": \"CCA ID\",\n",
        "    \"CC Agent_username\": \"CCA Name\",\n",
        "    \"CC Agent_email\": \"CCA Email\",\n",
        "    \"LO_id\": \"LO ID\",\n",
        "    \"LO_username\": \"LO Name\",\n",
        "    \"LO_email\": \"LO Email\",\n",
        "    \"Status\": \"ClickUp Status\",\n",
        "    \"School ID\": \"School ID\",\n",
        "    \"Task ID\": \"Task ID\",\n",
        "    \"List ID\": \"List ID\",\n",
        "    \"Project ID\": \"Project ID\",\n",
        "    \"Folder ID\": \"Folder ID\",\n",
        "    \"Space ID\": \"Space ID\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "pxN7lDLycQmZ"
      },
      "outputs": [],
      "source": [
        "# CLEAN : Decom & Swap\n",
        "\n",
        "def clean_DecomSwap(df, colnames):\n",
        "  df = process_status_column(df)\n",
        "  df = process_customer_type(df)\n",
        "  df = process_vertical_column(df)\n",
        "  df = process_brand_column(df)\n",
        "  df = fill_machine_delivery_date(df)\n",
        "  df = format_site_work(df)\n",
        "  df = fill_vertical_by_brand(df, brand_index)\n",
        "  df = fill_region_by_state(df, area_index)\n",
        "\n",
        "\n",
        "  # Filling in Empty Vertical, Customer Type, Brand:\n",
        "  index = create_index_table(df[['Vertical', 'Customer Type', 'Brand']])\n",
        "  df = autocomplete_table(df, index)\n",
        "\n",
        "  # Appending tag columns\n",
        "  tag_columns = [column for column in df.columns if \"Tag\" in column]\n",
        "\n",
        "  # Renaming and Reordering\n",
        "  df = df.rename(columns=colnames)  # Rename columns\n",
        "  df = df[list(list(colnames.values()) + tag_columns)]  # Reorder columns\n",
        "\n",
        "  return df\n",
        "\n",
        "DC_colnames = {\n",
        "    \"Location ID\": \"Location ID\",\n",
        "    \"Task Name\": \"Location Name\",\n",
        "    \"Vertical\": \"Vertical\",\n",
        "    \"Customer Type\": \"Customer Type\",\n",
        "    \"Brand\": \"Brand\",\n",
        "    \"Decom Date\": \"Decom Date\",\n",
        "    \"Launch Date\": \"Launch Date\",\n",
        "    \"Install Date\": \"Install Date\",\n",
        "    \"Machine delivery Date\": \"Machine Delivery Date\",\n",
        "    \"Survey Date\": \"Survey Date\",\n",
        "    \"Reason for Decommission\\t\": \"Decom Reason\",\n",
        "    \"Region\": \"Region\",\n",
        "    \"State\": \"State\",\n",
        "    \"formatted_address\": \"Formatted Address\",\n",
        "    \"lat\": \"Latitude\",\n",
        "    \"lng\": \"Longitude\",\n",
        "    \"place_id\": \"Place ID\",\n",
        "    \"Contract Type\": \"Contract Type\",\n",
        "    \"site work\": \"Site Work\",\n",
        "    \"Franchise Owner Name\": \"Franchise Owner Name\",\n",
        "    \"Franchise Owner Phone #\": \"Franchise Owner Phone\",\n",
        "    \"Franchise Owner Email\": \"Franchise Owner Email\",\n",
        "    \"Site GM Name\": \"Site GM Name\",\n",
        "    \"Site GM number\": \"Site GM Number\",\n",
        "    \"Site GM Email\": \"Site GM Email\",\n",
        "    \"Sales_id\": \"Sales ID\",\n",
        "    \"Sales_username\": \"Sales Name\",\n",
        "    \"Sales_email\": \"Sales Email\",\n",
        "    \"RAE_id\": \"RAE ID\",\n",
        "    \"RAE_username\": \"RAE Name\",\n",
        "    \"RAE_email\": \"RAE Email\",\n",
        "    \"SOE_id\": \"SOE ID\",\n",
        "    \"SOE_username\": \"SOE Name\",\n",
        "    \"SOE_email\": \"SOE Email\",\n",
        "    \"CC Agent_id\": \"CCA ID\",\n",
        "    \"CC Agent_username\": \"CCA Name\",\n",
        "    \"CC Agent_email\": \"CCA Email\",\n",
        "    \"LO_id\": \"LO ID\",\n",
        "    \"LO_username\": \"LO Name\",\n",
        "    \"LO_email\": \"LO Email\",\n",
        "    \"Status\": \"ClickUp Status\",\n",
        "    \"Task ID\": \"Task ID\",\n",
        "    \"List ID\": \"List ID\",\n",
        "    \"Project ID\": \"Project ID\",\n",
        "    \"Folder ID\": \"Folder ID\",\n",
        "    \"Space ID\": \"Space ID\"\n",
        "}\n",
        "\n",
        "# Clean with Other Data Sources: Contract Type, Area"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "WSJXS-OksPc_"
      },
      "outputs": [],
      "source": [
        "# CLEAN : Current Deployments\n",
        "\n",
        "def clean_CurrentDeployments(df, colnames):\n",
        "  df = process_status_column(df)\n",
        "  df = process_customer_type(df)\n",
        "  df = process_vertical_column(df)\n",
        "  df = process_brand_column(df)\n",
        "  df = fill_machine_delivery_date(df)\n",
        "  df = format_site_work(df)\n",
        "  df = fill_vertical_by_brand(df, brand_index)\n",
        "  df = fill_region_by_state(df, area_index)\n",
        "\n",
        "  # Appending tag columns\n",
        "  tag_columns = [column for column in df.columns if \"Tag\" in column]\n",
        "\n",
        "  # Renaming and Reordering\n",
        "  df = df.rename(columns=colnames)  # Rename columns\n",
        "  df = df[list(list(colnames.values()) + tag_columns)]  # Reorder columns\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "CD_colnames = {\n",
        "    \"Location ID\": \"Location ID\",\n",
        "    \"Task Name\": \"Location Name\",\n",
        "    \"Vertical\": \"Vertical\",\n",
        "    \"Customer Type\": \"Customer Type\",\n",
        "    \"Brand\": \"Brand\",\n",
        "    \"Tentative launch date\": \"Tentative Launch Date\",\n",
        "    \"On Track For Install date\": \"On Track For Install Date\",\n",
        "    \"Launch Date\": \"Launch Date\",\n",
        "    \"Install Date\": \"Install Date\",\n",
        "    \"Machine delivery Date\": \"Machine Delivery Date\",\n",
        "    \"Survey Date\": \"Survey Date\",\n",
        "    \"Region\": \"Region\",\n",
        "    \"State\": \"State\",\n",
        "    \"formatted_address\": \"Formatted Address\",\n",
        "    \"lat\": \"Latitude\",\n",
        "    \"lng\": \"Longitude\",\n",
        "    \"place_id\": \"Place ID\",\n",
        "    \"Operator\": \"Operator\",\n",
        "    \"Site Survey Options\": \"Site Survey Options\",\n",
        "    \"site work\": \"Site Work\",\n",
        "    \"Site GM Name\": \"Site GM Name\",\n",
        "    \"Site GM number\": \"Site GM Number\",\n",
        "    \"Site GM Email\": \"Site GM Email\",\n",
        "    \"Sales_id\": \"Sales ID\",\n",
        "    \"Sales_username\": \"Sales Name\",\n",
        "    \"Sales_email\": \"Sales Email\",\n",
        "    \"RAE_id\": \"RAE ID\",\n",
        "    \"RAE_username\": \"RAE Name\",\n",
        "    \"RAE_email\": \"RAE Email\",\n",
        "    \"SOE_id\": \"SOE ID\",\n",
        "    \"SOE_username\": \"SOE Name\",\n",
        "    \"SOE_email\": \"SOE Email\",\n",
        "    \"CC Agent_id\": \"CCA ID\",\n",
        "    \"CC Agent_username\": \"CCA Name\",\n",
        "    \"CC Agent_email\": \"CCA Email\",\n",
        "    \"LO_id\": \"LO ID\",\n",
        "    \"LO_username\": \"LO Name\",\n",
        "    \"LO_email\": \"LO Email\",\n",
        "    \"Status\": \"ClickUp Status\",\n",
        "    \"School ID\": \"School ID\",\n",
        "    \"Task ID\": \"Task ID\",\n",
        "    \"List ID\": \"List ID\",\n",
        "    \"Project ID\": \"Project ID\",\n",
        "    \"Folder ID\": \"Folder ID\",\n",
        "    \"Space ID\": \"Space ID\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rwjxczOGDSDg"
      },
      "outputs": [],
      "source": [
        "# CLEAN : Post Deployments\n",
        "\n",
        "def clean_PostDeployments(df, colnames):\n",
        "  df = process_status_column(df)\n",
        "  df = process_customer_type(df)\n",
        "  df = process_vertical_column(df)\n",
        "  df = process_brand_column(df)\n",
        "  df = fill_machine_delivery_date(df)\n",
        "  df = format_site_work(df)\n",
        "  df = fill_vertical_by_brand(df, brand_index)\n",
        "  df = fill_region_by_state(df, area_index)\n",
        "\n",
        "  # Appending tag columns\n",
        "  tag_columns = [column for column in df.columns if \"Tag\" in column]\n",
        "\n",
        "  # Renaming and Reordering\n",
        "  df = df.rename(columns=colnames)  # Rename columns\n",
        "  df = df[list(list(colnames.values()) + tag_columns)]  # Reorder columns\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "PD_colnames = {\n",
        "    \"Location ID\": \"Location ID\",\n",
        "    \"Task Name\": \"Location Name\",\n",
        "    \"Vertical\": \"Vertical\",\n",
        "    \"Customer Type\": \"Customer Type\",\n",
        "    \"Brand\": \"Brand\",\n",
        "    \"Tentative launch date\": \"Tentative Launch Date\",\n",
        "    \"On Track For Install date\": \"On Track For Install Date\",\n",
        "    \"Launch Date\": \"Launch Date\",\n",
        "    \"Install Date\": \"Install Date\",\n",
        "    \"Machine delivery Date\": \"Machine Delivery Date\",\n",
        "    \"Survey Date\": \"Survey Date\",\n",
        "    \"Region\": \"Region\",\n",
        "    \"State\": \"State\",\n",
        "    \"formatted_address\": \"Formatted Address\",\n",
        "    \"lat\": \"Latitude\",\n",
        "    \"lng\": \"Longitude\",\n",
        "    \"place_id\": \"Place ID\",\n",
        "    \"Operator\": \"Operator\",\n",
        "    \"Site Survey Options\": \"Site Survey Options\",\n",
        "    \"site work\": \"Site Work\",\n",
        "    \"Site GM Name\": \"Site GM Name\",\n",
        "    \"Site GM number\": \"Site GM Number\",\n",
        "    \"Site GM Email\": \"Site GM Email\",\n",
        "    \"Sales_id\": \"Sales ID\",\n",
        "    \"Sales_username\": \"Sales Name\",\n",
        "    \"Sales_email\": \"Sales Email\",\n",
        "    \"RAE_id\": \"RAE ID\",\n",
        "    \"RAE_username\": \"RAE Name\",\n",
        "    \"RAE_email\": \"RAE Email\",\n",
        "    \"SOE_id\": \"SOE ID\",\n",
        "    \"SOE_username\": \"SOE Name\",\n",
        "    \"SOE_email\": \"SOE Email\",\n",
        "    \"CC Agent_id\": \"CCA ID\",\n",
        "    \"CC Agent_username\": \"CCA Name\",\n",
        "    \"CC Agent_email\": \"CCA Email\",\n",
        "    \"LO_id\": \"LO ID\",\n",
        "    \"LO_username\": \"LO Name\",\n",
        "    \"LO_email\": \"LO Email\",\n",
        "    \"Status\": \"ClickUp Status\",\n",
        "    \"School ID\": \"School ID\",\n",
        "    \"Task ID\": \"Task ID\",\n",
        "    \"List ID\": \"List ID\",\n",
        "    \"Project ID\": \"Project ID\",\n",
        "    \"Folder ID\": \"Folder ID\",\n",
        "    \"Space ID\": \"Space ID\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "u3Uer4hoMMKO"
      },
      "outputs": [],
      "source": [
        "# CLEAN : Plumbing Job\n",
        "\n",
        "def clean_PlumbingJob(df, colnames):\n",
        "  df = process_status_column(df)\n",
        "  df = process_customer_type(df)\n",
        "\n",
        "  if \"Vertical\" in df.columns:\n",
        "    df = process_vertical_column(df)\n",
        "\n",
        "  df = process_brand_column(df)\n",
        "\n",
        "  if \"State\" in df.columns:\n",
        "    df = fill_region_by_state(df, area_index)\n",
        "\n",
        "  # Appending tag columns\n",
        "  tag_columns = [column for column in df.columns if \"Tag\" in column]\n",
        "\n",
        "  # Renaming and Reordering\n",
        "  df = df.rename(columns=colnames)  # Rename columns\n",
        "  df = df[list(list(colnames.values()) + tag_columns)]  # Reorder columns\n",
        "\n",
        "  return df\n",
        "\n",
        "PJ_colnames = plumbingjob_colnames = {\n",
        "    \"Task ID\": \"Task ID\",\n",
        "    \"Task Name\": \"Location Name\",\n",
        "    \"Customer Type\": \"Customer Type\",\n",
        "    \"Brand\": \"Brand\",\n",
        "    \"SS Report\": \"SS Report\",\n",
        "    \"Plumbing Task\": \"Plumbing Task\",\n",
        "    \"Other Plumbing job\": \"Other Plumbing Job\",\n",
        "    \"Plumber type\": \"Plumber Type\",\n",
        "    \"Quote\": \"Quote\",\n",
        "    \"Initial Cost\": \"Initial Cost\",\n",
        "    \"Final Cost\": \"Final Cost\",\n",
        "    \"Plumber ETA\": \"Plumber ETA\",\n",
        "    \"Install time ETA\": \"Install Time ETA\",\n",
        "    \"place_id\": \"Place ID\",\n",
        "    \"lat\": \"Latitude\",\n",
        "    \"lng\": \"Longitude\",\n",
        "    \"formatted_address\": \"Formatted Address\",\n",
        "    #\"Region\": \"Region\",\n",
        "    \"Status\": \"ClickUp Status\",\n",
        "    \"Sales_id\": \"Sales ID\",\n",
        "    \"Sales_username\": \"Sales Name\",\n",
        "    \"Sales_email\": \"Sales Email\",\n",
        "    \"Coordinator_id\": \"LO ID\",\n",
        "    \"Coordinator_username\": \"LO Name\",\n",
        "    \"Coordinator_email\": \"LO Email\",\n",
        "    \"CC Agent_id\": \"CCA ID\",\n",
        "    \"CC Agent_username\": \"CCA Name\",\n",
        "    \"CC Agent_email\": \"CCA Email\",\n",
        "    \"SOE_id\": \"SOE ID\",\n",
        "    \"SOE_username\": \"SOE Name\",\n",
        "    \"SOE_email\": \"SOE Email\",\n",
        "    \"List ID\": \"List ID\",\n",
        "    \"Project ID\": \"Project ID\",\n",
        "    \"Folder ID\": \"Folder ID\",\n",
        "    \"Space ID\": \"Space ID\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Hd6u-_nZJw"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7YQjoAkzjiag"
      },
      "outputs": [],
      "source": [
        "# GoogleDrive\n",
        "def load_GoogleDrive(df, path):\n",
        "    \"\"\"Saves a DataFrame as a CSV file in Google Drive.\"\"\"\n",
        "    df.to_csv(path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"Saved to: {path}\")\n",
        "    return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "27jiK6syohUr"
      },
      "outputs": [],
      "source": [
        "# GoogleDrive\n",
        "def load_GoogleDrive(df, path):\n",
        "  df.to_csv(path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uQ20X46cteT"
      },
      "source": [
        "## ETL Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JFiwtKyH3vAK"
      },
      "outputs": [],
      "source": [
        "#EXTRACT\n",
        "\n",
        "def extract_ClickUp(list_id):\n",
        "\n",
        "  ### Getting the raw list of tasks using a list id\n",
        "  Tasklist_raw = get_all_tasks(list_id)\n",
        "\n",
        "  # Extracting task ids from raw data\n",
        "  Tasklist_IDs = extract_task_ids(Tasklist_raw)\n",
        "\n",
        "  # Getting the raw task details and extracting the columns from raw data\n",
        "  Tasklist_all = extract_task_details(Tasklist_IDs)\n",
        "\n",
        "  # Flatenning nested data into a structured dataframe\n",
        "  Tasklist_flat = flatten_nested_data(Tasklist_all)\n",
        "\n",
        "  ### Converting to DataFrame\n",
        "  Tasklist = pd.DataFrame(Tasklist_flat)\n",
        "  return Tasklist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HWADz0Z0HM51"
      },
      "outputs": [],
      "source": [
        "#TRANSFORM\n",
        "\n",
        "def transform_ClickUp(df, list_id):\n",
        "\n",
        "  # Removing column and rows composed mostly of NA values\n",
        "  df_lean = lean_dataframe(df, 1, 10)\n",
        "\n",
        "  ### Getting custom field information\n",
        "  CustomFields_raw = get_custom_fields_from_list(list_id)\n",
        "  CustomFields = extract_custom_fields(CustomFields_raw)\n",
        "\n",
        "  # One-hot encoding tag columns\n",
        "  if \"Tags\" in df_lean.columns:\n",
        "    df_lean = expand_tags_column(df_lean, \"Tags\")\n",
        "\n",
        "  # Standardizing the data types for all columns\n",
        "  df_typed = standardize_column_types(df_lean, column_names=CustomFields['Name'], column_types=CustomFields['Type'])\n",
        "\n",
        "  ### Build lookup table for indexed columns\n",
        "  lookup_table = build_lookup(CustomFields_raw)\n",
        "\n",
        "  # Indexing all indexed columns to display real values\n",
        "  df_indexed = replace_indexes(df_typed, lookup_table)\n",
        "\n",
        "  return df_indexed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjb8gk8vjYzj"
      },
      "source": [
        "# Data Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkKzTJoGkQ5K"
      },
      "source": [
        "#### Live Machines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thojqmMt5l8p",
        "outputId": "e4c1bb28-3229-4b80-9a78-c092721480b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching Tasks: 11page [00:31,  2.82s/page]\n",
            "Processing Tasks: 100%|██████████| 1032/1032 [07:37<00:00,  2.26task/s]\n"
          ]
        }
      ],
      "source": [
        "LiveMachinesE = extract_ClickUp(LiveMachines_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OVPTQa-wkOwq"
      },
      "outputs": [],
      "source": [
        "LiveMachinesT = transform_ClickUp(LiveMachinesE, LiveMachines_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IdB4acl6PJ9m"
      },
      "outputs": [],
      "source": [
        "LiveMachinesC = clean_LiveMachines(LiveMachinesT, LM_colnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eXBb5uKGkk3x"
      },
      "outputs": [],
      "source": [
        "path = \"drive/MyDrive/02 Areas/Business Intelligence/Datasets/LiveMachines.csv\"\n",
        "load_GoogleDrive(LiveMachinesC, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg740K43kS4u"
      },
      "source": [
        "#### Decom & Swap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KhmgIWu-OWe",
        "outputId": "5bda33e4-ac57-4bc9-f291-9c4b42486289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching Tasks: 5page [00:12,  2.46s/page]\n",
            "Processing Tasks: 100%|██████████| 496/496 [03:35<00:00,  2.30task/s]\n"
          ]
        }
      ],
      "source": [
        "DecomSwapE = extract_ClickUp(DecomSwap_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TBmF7Qp9kXPU"
      },
      "outputs": [],
      "source": [
        "DecomSwapT = transform_ClickUp(DecomSwapE, DecomSwap_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHHrGpC_jgpn",
        "outputId": "05257c3c-5547-4a9d-dc0a-a1d517afaefb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-bbbfcee9eb3f>:90: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Machine delivery Date\"] = df[available_cols].bfill(axis=1).iloc[:, 0]\n"
          ]
        }
      ],
      "source": [
        "DecomSwapC = clean_DecomSwap(DecomSwapT, DC_colnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "1eD_su9-RK68"
      },
      "outputs": [],
      "source": [
        "path = \"drive/MyDrive/02 Areas/Business Intelligence/Datasets/DecomSwap.csv\"\n",
        "load_GoogleDrive(DecomSwapC, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdZcwxipkVEC"
      },
      "source": [
        "#### Current Deployments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WlJXg35WvsX-",
        "outputId": "f862e673-c044-44a5-8167-9853b9215ccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching Tasks: 4page [00:10,  2.63s/page]\n",
            "Processing Tasks: 100%|██████████| 319/319 [02:39<00:00,  2.00task/s]\n"
          ]
        }
      ],
      "source": [
        "CurrentDeploymentsE = extract_ClickUp(CurrentDeployments_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "uXddypEAJFDH"
      },
      "outputs": [],
      "source": [
        "CurrentDeploymentsT = transform_ClickUp(CurrentDeploymentsE, CurrentDeployments_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "o788oYLJspA4",
        "outputId": "e62845b9-0ad1-4896-e607-8fda68a3ab70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-bbbfcee9eb3f>:90: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[\"Machine delivery Date\"] = df[available_cols].bfill(axis=1).iloc[:, 0]\n"
          ]
        }
      ],
      "source": [
        "CurrentDeploymentsC = clean_CurrentDeployments(CurrentDeploymentsT, CD_colnames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ynrmJ9DfEzww"
      },
      "outputs": [],
      "source": [
        "path = \"drive/MyDrive/02 Areas/Business Intelligence/Datasets/CurrentDeployments.csv\"\n",
        "load_GoogleDrive(CurrentDeploymentsT, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blamoOawkYNG"
      },
      "source": [
        "#### Post Deployments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umO_klWUXwF5",
        "outputId": "cfba164b-9270-43fa-f3ec-eeccab7a7113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching Tasks: 0page [00:00, ?page/s]\n",
            "Processing Tasks: 0task [00:00, ?task/s]\n"
          ]
        }
      ],
      "source": [
        "PostDeploymentsE = extract_ClickUp(PostDeployments_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "AJNlBIXdkbAy"
      },
      "outputs": [],
      "source": [
        "PostDeploymentsT = transform_ClickUp(PostDeploymentsE, PostDeployments_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLxT592vK7aF"
      },
      "outputs": [],
      "source": [
        "PostDeploymentsC = clean_PostDeployments(PostDeploymentsT, PD_colnames)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/02 Areas/Business Intelligence/Datasets/PostDeployments.csv\"\n",
        "load_GoogleDrive(PostDeploymentsC, path)"
      ],
      "metadata": {
        "id": "RrXKUWi1jaAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc1RHg1Ykb8H"
      },
      "source": [
        "#### Plumbing Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33wcXy9EXRyB",
        "outputId": "0819cfd3-0ad5-4f1f-c7c9-570cd37a5350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching Tasks: 1page [00:01,  1.66s/page]\n",
            "Processing Tasks: 100%|██████████| 58/58 [00:25<00:00,  2.29task/s]\n"
          ]
        }
      ],
      "source": [
        "PlumbingJobE = extract_ClickUp(PlumbingJob_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ceSV-PeckeWS"
      },
      "outputs": [],
      "source": [
        "PlumbingJobT = transform_ClickUp(PlumbingJobE, PlumbingJob_LISTID)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PlumbingJobC = clean_PlumbingJob(PlumbingJobT, PJ_colnames)"
      ],
      "metadata": {
        "id": "Z2VN3vXeTkqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/02 Areas/Business Intelligence/Datasets/PlumbingJob.csv\"\n",
        "load_GoogleDrive(PlumbingJobC, path)"
      ],
      "metadata": {
        "id": "jnPINh2LjigQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zZRe4SxU2GO"
      },
      "source": [
        "# Dataset Building\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNip98tgJ6A-"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)  # Show all rows\n",
        "pd.set_option('display.max_columns', None)  # Show all columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove duplicate column names\n",
        "LiveMachinesM = LiveMachinesC.loc[:, ~LiveMachinesC.columns.duplicated()]\n",
        "DecomSwapM = DecomSwapC.loc[:, ~DecomSwapC.columns.duplicated()]\n",
        "CurrentDeploymentsM = CurrentDeploymentsC.loc[:, ~CurrentDeploymentsC.columns.duplicated()]\n",
        "PostDeploymentsM = PostDeploymentsC.loc[:, ~PostDeploymentsC.columns.duplicated()]\n",
        "PlumbingJobM = PlumbingJobC.loc[:, ~PlumbingJobC.columns.duplicated()]\n",
        "\n",
        "# Step 2: Reset index to avoid duplicate row indices\n",
        "LiveMachinesM = LiveMachinesM.reset_index(drop=True)\n",
        "DecomSwapM = DecomSwapM.reset_index(drop=True)\n",
        "PostDeploymentsM = PostDeploymentsM.reset_index(drop=True)\n",
        "CurrentDeploymentsM = CurrentDeploymentsM.reset_index(drop=True)\n",
        "PlumbingJobM = PlumbingJobM.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "KYH68jqNk79t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AllClickUp Dataset"
      ],
      "metadata": {
        "id": "n1f7wTtskQps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Merging all ClickUp datasets === #\n",
        "\n",
        "# Concatenate all datasets using union of columns\n",
        "AllClickUp = pd.concat(\n",
        "    [LiveMachinesM, CurrentDeploymentsM, DecomSwapM, PostDeploymentsM, PlumbingJobM],\n",
        "    ignore_index=True,\n",
        "    sort=False\n",
        ")\n",
        "\n",
        "# Optional: Preview the combined DataFrame\n",
        "print(AllClickUp.shape)\n",
        "print(AllClickUp.columns.tolist())"
      ],
      "metadata": {
        "id": "vX1CJcFFY6GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Labeling for all ClickUp statuses === #\n",
        "live_statuses = ['Enterprise Park ', 'Mid Market', 'Enterprise Stage 3&4','Channel Partner', 'Enterprise Stage 2']\n",
        "\n",
        "decom_statuses = ['Decommission', 'Removal', 'Removal Audit']\n",
        "\n",
        "deploy_statuses = ['Not Scheduled', 'Scheduled For Install', 'Partially Installed', 'Equipment Delivered Only', 'To Be Invoiced']\n",
        "\n",
        "swap_statuses = ['Swap', 'Swapped', 'Relocation']\n",
        "\n",
        "plumbing_statuses = ['Scheduled For Work', 'Hold', 'In Progress', 'Cancelled', 'Quoted', 'Delayed']\n",
        "\n",
        "# Now all leftovers go to \"other\"\n",
        "all_statuses = set(AllClickUp['ClickUp Status'].unique())\n",
        "\n",
        "categorized_statuses = set(\n",
        "    live_statuses + decom_statuses + deploy_statuses + swap_statuses + plumbing_statuses\n",
        ")\n",
        "\n",
        "other_statuses = sorted(all_statuses - categorized_statuses)\n",
        "\n",
        "\n",
        "# === Subsetting Task IDs by ClickUp status === #\n",
        "live_IDs = AllClickUp[AllClickUp['ClickUp Status'].isin(live_statuses)]['Task ID'].tolist()\n",
        "decom_IDs = AllClickUp[AllClickUp['ClickUp Status'].isin(decom_statuses)]['Task ID'].tolist()\n",
        "deploy_IDs = AllClickUp[AllClickUp['ClickUp Status'].isin(deploy_statuses)]['Task ID'].tolist()\n",
        "swap_IDs = AllClickUp[AllClickUp['ClickUp Status'].isin(swap_statuses)]['Task ID'].tolist()\n",
        "plumbing_IDs = AllClickUp[AllClickUp['ClickUp Status'].isin(plumbing_statuses)]['Task ID'].tolist()\n",
        "other_IDs = AllClickUp[AllClickUp['ClickUp Status'].isin(other_statuses)]['Task ID'].tolist()\n",
        "\n",
        "print(f'Live ID elements: {len(live_IDs)}')\n",
        "print(f'Decom ID elements: {len(decom_IDs)}')\n",
        "print(f'Deploy ID elements: {len(deploy_IDs)}')\n",
        "print(f'Swap ID elements: {len(swap_IDs)}')\n",
        "print(f'Plumbing ID elements: {len(plumbing_IDs)}')\n",
        "print(f'Other ID elements: {len(other_IDs)}')\n",
        "\n",
        "print(f'\\nStatus ID objects are type: {type(other_IDs)}')"
      ],
      "metadata": {
        "id": "NTdla8v_LOV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Labeling by Analytical Category === #\n",
        "\n",
        "AllClickUp['Status'] = np.where(AllClickUp['Task ID'].isin(live_IDs), 'Live',\n",
        "                      np.where(AllClickUp['Task ID'].isin(decom_IDs), 'Decommission',\n",
        "                      np.where(AllClickUp['Task ID'].isin(deploy_IDs), 'Deployment',\n",
        "                      np.where(AllClickUp['Task ID'].isin(swap_IDs), 'Swapped',\n",
        "                      np.where(AllClickUp['Task ID'].isin(plumbing_IDs), 'Plumbing',\n",
        "                      np.where(AllClickUp['Task ID'].isin(other_IDs), 'Other',''))))))\n",
        "\n",
        "AllClickUp['Status'].value_counts()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zSuC_mEcZBAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_colorder = [\n",
        "\n",
        "    # --- Core Identifiers ---\n",
        "    'Location ID',\n",
        "    'Location Name',\n",
        "    'Vertical',\n",
        "    'Customer Type',\n",
        "    'Brand',\n",
        "    'Status',\n",
        "\n",
        "    # --- Geography ---\n",
        "    'Region',\n",
        "    'State',\n",
        "    'Formatted Address',\n",
        "    'Latitude',\n",
        "    'Longitude',\n",
        "    'Place ID',\n",
        "\n",
        "    # --- Dates ---\n",
        "    'Launch Date',\n",
        "    'Install Date',\n",
        "    'Machine Delivery Date',\n",
        "    'Survey Date',\n",
        "    'Tentative Launch Date',\n",
        "    'On Track For Install Date',\n",
        "    'Decom Date',\n",
        "    'Plumber ETA',\n",
        "    'Install Time ETA',\n",
        "\n",
        "    # --- People Columns ---\n",
        "    'Operator',\n",
        "    'Site GM Name',\n",
        "    'Site GM Number',\n",
        "    'Site GM Email',\n",
        "    'Sales ID',\n",
        "    'Sales Name',\n",
        "    'Sales Email',\n",
        "    'RAE ID',\n",
        "    'RAE Name',\n",
        "    'RAE Email',\n",
        "    'SOE ID',\n",
        "    'SOE Name',\n",
        "    'SOE Email',\n",
        "    'CCA ID',\n",
        "    'CCA Name',\n",
        "    'CCA Email',\n",
        "    'LO ID',\n",
        "    'LO Name',\n",
        "    'LO Email',\n",
        "    'Franchise Owner Name',\n",
        "    'Franchise Owner Phone',\n",
        "    'Franchise Owner Email',\n",
        "\n",
        "    # --- Other Fields ---\n",
        "    'Decom Reason',\n",
        "    'Contract Type',\n",
        "    'Site Work',\n",
        "    'SS Report',\n",
        "    'Plumbing Task',\n",
        "    'Other Plumbing Job',\n",
        "    'Plumber Type',\n",
        "    'Quote',\n",
        "    'Initial Cost',\n",
        "    'Final Cost',\n",
        "    'Site Survey Options',\n",
        "\n",
        "    # --- ClickUp IDs ---\n",
        "\n",
        "    'ClickUp Status',\n",
        "    'School ID',\n",
        "    'Task ID',\n",
        "    'List ID',\n",
        "    'Project ID',\n",
        "    'Folder ID',\n",
        "    'Space ID',\n",
        "\n",
        "    # --- Tags ---\n",
        "    'Tags',\n",
        "    '[can schedule site work] Tag',\n",
        "    '[q2] Tag',\n",
        "    '[partially installed] Tag',\n",
        "    '[new construction] Tag',\n",
        "    '[t1] Tag',\n",
        "    '[post sq] Tag',\n",
        "    '[backflow] Tag',\n",
        "    '[case study] Tag',\n",
        "    '[adjustment] Tag',\n",
        "    '[cca order verified] Tag',\n",
        "    '[need plumbing scheduled] Tag',\n",
        "    '[pop delay] Tag',\n",
        "    '[need to review sq] Tag',\n",
        "    '[high priority] Tag',\n",
        "    '[assignment test] Tag',\n",
        "    '[incomplete] Tag',\n",
        "    '[launch tracker] Tag',\n",
        "    '[tentative install set] Tag',\n",
        "    '[on track for launch] Tag',\n",
        "    '[on track for install] Tag',\n",
        "    '[risk of churn] Tag',\n",
        "    '[can schedule install/launch] Tag',\n",
        "    '[plumbing] Tag',\n",
        "    '[no sq report] Tag',\n",
        "    '[ice machine needed] Tag',\n",
        "    '[email] Tag',\n",
        "    '[important location] Tag',\n",
        "    '[conversion] Tag',\n",
        "    '[relaunch] Tag',\n",
        "    '[delay install/launch] Tag',\n",
        "    '[need follow up] Tag',\n",
        "    '[setup] Tag',\n",
        "    '[hold] Tag',\n",
        "    '[not scheduled] Tag',\n",
        "    '[new!] Tag',\n",
        "    '[bib adaptor] Tag',\n",
        "    '[compass] Tag',\n",
        "    '[sales calendly] Tag',\n",
        "    '[sq/vsq to be scheduled] Tag',\n",
        "    '[site work in progress] Tag',\n",
        "    '[cloudbar only] Tag',\n",
        "    '[delivered only] Tag',\n",
        "    '[r&d bot] Tag',\n",
        "    '[hawaii] Tag',\n",
        "    '[tlds] Tag',\n",
        "    '[waiting for contract] Tag',\n",
        "    '[ghosting franchisee] Tag',\n",
        "    '[sq report in progress] Tag',\n",
        "    '[order added] Tag',\n",
        "    '[q4] Tag'\n",
        "]\n",
        "\n",
        "AllClickUp = AllClickUp[all_colorder]"
      ],
      "metadata": {
        "id": "nYpOcO_ym7hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"drive/MyDrive/02 Areas/Business Intelligence/Datasets/AllClickUp.csv\"\n",
        "load_GoogleDrive(AllClickUp, path)"
      ],
      "metadata": {
        "id": "kMpDkelKldSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LiveDecom Dataset"
      ],
      "metadata": {
        "id": "C0TllT4CkfPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Building the << LiveDecom >> dataset === #\n",
        "LiveDecom = AllClickUp[(AllClickUp['Task ID'].isin(live_IDs)) | (AllClickUp['Task ID'].isin(decom_IDs))]"
      ],
      "metadata": {
        "id": "bfjQhKoVf1mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG_Ej4h0DeEB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# === Merging all ClickUp datasets === #\n",
        "\n",
        "# Step 1: Remove duplicate column names\n",
        "LiveMachinesM = LiveMachinesC.loc[:, ~LiveMachinesC.columns.duplicated()]\n",
        "DecomSwapM = DecomSwapC.loc[:, ~DecomSwapC.columns.duplicated()]\n",
        "\n",
        "\n",
        "# Step 2: Reset index to avoid duplicate row indices\n",
        "LiveMachinesM = LiveMachinesM.reset_index(drop=True)\n",
        "DecomSwapM = DecomSwapM.reset_index(drop=True)\n",
        "\n",
        "# Step 3: Align columns before concatenation\n",
        "common_cols = LiveMachinesC.columns.intersection(DecomSwapC.columns)\n",
        "\n",
        "# Option 1: Use only shared columns\n",
        "LiveDecom = pd.concat([LiveMachinesC[common_cols], DecomSwapC[common_cols]], ignore_index=True)\n",
        "\n",
        "# Option 2: Keep all columns (even if missing in one DataFrame)\n",
        "LiveDecom = pd.concat([LiveMachinesM, DecomSwapM], ignore_index=True, sort=False)\n",
        "\n",
        "# Print or return the final DataFrame\n",
        "print(LiveDecom)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(AllClickUp['ClickUp Status'])"
      ],
      "metadata": {
        "id": "WrfiRI_7esEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AllClickUp['ClickUp Status'].unique()"
      ],
      "metadata": {
        "id": "TcJ2DYgKe4lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgreCRq-ETsz"
      },
      "outputs": [],
      "source": [
        "LD_cols = ['Location ID', 'Location Name', 'Vertical', 'Customer Type', 'Brand',\n",
        "       'Decom Date','Launch Date', 'Install Date', 'Machine Delivery Date', 'Survey Date',\n",
        "       'Area', 'State', 'Formatted Address', 'Latitude', 'Longitude','Place ID',\n",
        "        'Contract Type', 'Site Work', 'Site GM Name','Sales Name',\n",
        "       'RAE Name', 'SOE Name', 'CCA Name', 'LO Name', 'Franchise Owner Name',\n",
        "        'ClickUp Status', 'School ID', 'Task ID', 'List ID', 'Project ID', 'Folder ID', 'Space ID']\n",
        "\n",
        "LiveDecom = LiveDecom[LD_cols]\n",
        "\n",
        "load_GoogleDrive(LiveDecom, \"drive/MyDrive/02 Areas/Business Intelligence/Datasets/LiveDecom (02-04-2025).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DecomSwapE['Vertical']"
      ],
      "metadata": {
        "id": "ULwuOeYRuRWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yR23-sCCUtO"
      },
      "outputs": [],
      "source": [
        "# === Concatenating Datasets === #\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def clean_and_concat(dataframes, keep_all_columns=True):\n",
        "    \"\"\"\n",
        "    Cleans multiple DataFrames by removing duplicate columns, resetting index,\n",
        "    and concatenating them based on shared or all columns.\n",
        "\n",
        "    Parameters:\n",
        "        dataframes (list of pd.DataFrame): List of DataFrames to process.\n",
        "        keep_all_columns (bool): If True, keep all columns; if False, keep only shared columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The concatenated DataFrame.\n",
        "    \"\"\"\n",
        "    # Step 1: Remove duplicate column names\n",
        "    cleaned_dfs = [df.loc[:, ~df.columns.duplicated()] for df in dataframes]\n",
        "\n",
        "    # Step 2: Reset index\n",
        "    cleaned_dfs = [df.reset_index(drop=True) for df in cleaned_dfs]\n",
        "\n",
        "    if keep_all_columns:\n",
        "        # Option 2: Keep all columns (even if missing in one DataFrame)\n",
        "        result = pd.concat(cleaned_dfs, ignore_index=True, sort=False)\n",
        "    else:\n",
        "        # Option 1: Use only shared columns\n",
        "        common_cols = set.intersection(*(set(df.columns) for df in cleaned_dfs))\n",
        "        result = pd.concat([df[common_cols] for df in cleaned_dfs], ignore_index=True)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbwHnwTpN3Sk"
      },
      "outputs": [],
      "source": [
        "df_list = [LiveMachinesC, DecomSwapC, CurrentDeploymentsC, PostDeploymentsC]\n",
        "AllClickUp = clean_and_concat(df_list, keep_all_columns=True)  # Change to False to use only shared columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsZSkQRcOFZn"
      },
      "outputs": [],
      "source": [
        "load_GoogleDrive(AllClickUp, \"drive/MyDrive/02 Areas/Business Intelligence/Datasets/AllClickUp.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZdGhEYIO9gs"
      },
      "outputs": [],
      "source": [
        "len(AllClickUp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBZ_ELZ5cwA3"
      },
      "source": [
        "# Playground: Start Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKluakKVtT5O"
      },
      "source": [
        "To Clean: Status, site work, Accessories & Toppings, Types of Accessories and Toppings, Customer Type, Brand, Vertical, Site Survey Options\n",
        "\n",
        "Filled with Nan: Tentative Launch Date, Location ID,\n",
        "\n",
        "Remove: , Launch Week\n",
        "\n",
        "Keep: On Track for Install date, Menu, Survey Date, Launch Date\n",
        "\n",
        "Fill in: Machine Type (fill with cloudbar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yQoO2FHs68Q"
      },
      "outputs": [],
      "source": [
        "LiveMachinesC[['Location Name', 'Formatted Address', 'Latitude', 'Longitude']].iloc[130:140]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL7hNY-bt-V9"
      },
      "outputs": [],
      "source": [
        "CurrentDeploymentsT['LO_email']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aFCYawDKN2jh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.auth import default\n",
        "\n",
        "# Function to load DataFrame into a specific Google Sheets worksheet\n",
        "def load_dataframe_to_google_sheets(df, spreadsheet_name, worksheet_name):\n",
        "    # Authenticate the user (This step will prompt for authorization in Google Colab)\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    # Use google-auth to get credentials and authorize the client\n",
        "    creds, _ = default()  # This gets the default credentials\n",
        "    client = gspread.authorize(creds)  # Authorize using google-auth credentials\n",
        "\n",
        "    # Clean the DataFrame: Replace NaN, Inf, and -Inf\n",
        "    df_cleaned = df.replace([float('inf'), float('-inf')], None)  # Replace Inf and -Inf with None (JSON-compliant)\n",
        "    df_cleaned = df_cleaned.fillna('')  # Replace NaN values with 'N/A'\n",
        "\n",
        "    # Convert the entire dataframe to string to ensure no issues with non-serializable objects\n",
        "    df_cleaned = df_cleaned.astype(str)\n",
        "\n",
        "    # Open the spreadsheet\n",
        "    sheet = client.open(spreadsheet_name)\n",
        "\n",
        "    # Try to get the worksheet by name, if it exists, delete it\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(worksheet_name)  # Try to access the existing worksheet\n",
        "        sheet.del_worksheet(worksheet)  # Delete the worksheet if it exists\n",
        "        print(f\"Deleted existing worksheet: {worksheet_name}\")\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(f\"Worksheet '{worksheet_name}' not found, proceeding to create it.\")\n",
        "\n",
        "    # Create a new worksheet with the name and dimensions based on the DataFrame\n",
        "    new_worksheet = sheet.add_worksheet(title=worksheet_name, rows=df_cleaned.shape[0] + 1, cols=df_cleaned.shape[1])\n",
        "    print(f\"Created new worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Upload the updated DataFrame to the new worksheet\n",
        "    new_worksheet.update([df_cleaned.columns.values.tolist()] + df_cleaned.values.tolist())\n",
        "    print(f\"Uploaded new data to worksheet: {worksheet_name}\")\n",
        "\n",
        "# Example usage\n",
        "spreadsheet_name = \"Botrista Datasets\"  # Replace with your actual spreadsheet name\n",
        "worksheet_name = \"Live Machines\"  # Replace with your actual worksheet name\n",
        "\n",
        "# Assuming 'LiveMachinesC' is your DataFrame\n",
        "load_dataframe_to_google_sheets(LiveMachinesC, spreadsheet_name, worksheet_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-adhGpTSc42"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.auth.transport.requests import Request\n",
        "from google.auth import default\n",
        "\n",
        "# Function to load DataFrame into a specific Google Sheets worksheet\n",
        "def load_dataframe_to_google_sheets(df, spreadsheet_name, worksheet_name):\n",
        "    # Authenticate the user (This step will prompt for authorization in Google Colab)\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    # Use google-auth to get credentials and authorize the client\n",
        "    creds, _ = default()  # This gets the default credentials\n",
        "    client = gspread.authorize(creds)  # Authorize using google-auth credentials\n",
        "\n",
        "    # Clean the DataFrame: Replace NaN, Inf, and -Inf\n",
        "    df_cleaned = df.replace([float('inf'), float('-inf')], None)  # Replace Inf and -Inf with None (JSON-compliant)\n",
        "    df_cleaned = df_cleaned.fillna('')  # Replace NaN values with empty strings\n",
        "\n",
        "    # Convert datetime columns to string format (e.g., 'YYYY-MM-DD')\n",
        "    for col in df_cleaned.select_dtypes(include=['datetime']).columns:\n",
        "        df_cleaned[col] = df_cleaned[col].dt.strftime('%Y-%m-%d')  # You can adjust the format if needed\n",
        "\n",
        "    # Open the spreadsheet\n",
        "    sheet = client.open(spreadsheet_name)\n",
        "\n",
        "    # Try to get the worksheet by name, if it exists, delete it\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(worksheet_name)  # Try to access the existing worksheet\n",
        "        sheet.del_worksheet(worksheet)  # Delete the worksheet if it exists\n",
        "        print(f\"Deleted existing worksheet: {worksheet_name}\")\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(f\"Worksheet '{worksheet_name}' not found, proceeding to create it.\")\n",
        "\n",
        "    # Create a new worksheet with the name and dimensions based on the DataFrame\n",
        "    new_worksheet = sheet.add_worksheet(title=worksheet_name, rows=df_cleaned.shape[0] + 1, cols=df_cleaned.shape[1])\n",
        "    print(f\"Created new worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Upload the updated DataFrame to the new worksheet\n",
        "    new_worksheet.update([df_cleaned.columns.values.tolist()] + df_cleaned.values.tolist())\n",
        "    print(f\"Uploaded new data to worksheet: {worksheet_name}\")\n",
        "\n",
        "# Example usage\n",
        "spreadsheet_name = \"Botrista Datasets\"  # Replace with your actual spreadsheet name\n",
        "worksheet_name = \"Live Machines\"  # Replace with your actual worksheet name\n",
        "\n",
        "# Assuming 'LiveMachinesC' is your DataFrame\n",
        "load_dataframe_to_google_sheets(LiveMachinesC[['Customer Type','Area','Launch Date']], spreadsheet_name, worksheet_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5gD7ycScqlK"
      },
      "source": [
        "## Automation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjDi94As4Ntu"
      },
      "outputs": [],
      "source": [
        "# AUTOMATION : Outreach and notifications through emails and messages.\n",
        "\n",
        "# Function to create a new task\n",
        "def create_task(list_id, task_name, description=\"\"):\n",
        "    url = f\"https://api.clickup.com/api/v2/list/{list_id}/task\"\n",
        "    payload = {\n",
        "        \"name\": task_name,\n",
        "        \"description\": description\n",
        "    }\n",
        "    response = requests.post(url, json=payload, headers=HEADERS)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "# Fetches all comments for a task\n",
        "def get_comments(task_id):\n",
        "  url = \"https://api.clickup.com/api/v2/task/task_id/comment\"\n",
        "  response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U3v16Q8AE_4-"
      },
      "outputs": [],
      "source": [
        "# GoogleSheets\n",
        "def load_GoogleSheets(df, spreadsheet_name, worksheet_name):\n",
        "    # Authenticate the user (This step will prompt for authorization in Google Colab)\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    # Use google-auth to get credentials and authorize the client\n",
        "    creds, _ = default()  # This gets the default credentials\n",
        "    client = gspread.authorize(creds)  # Authorize using google-auth credentials\n",
        "\n",
        "    # Clean the DataFrame to handle problematic values (NaN, Inf, -Inf)\n",
        "    df_cleaned = df.replace([float('inf'), float('-inf')], None)  # Replace Inf and -Inf with None (JSON-compliant)\n",
        "    df_cleaned = df_cleaned.fillna('')  # Replace NaN values with 'N/A' or any placeholder value you prefer\n",
        "\n",
        "    # Convert all datetime or date columns to string format\n",
        "    for column in df_cleaned.select_dtypes(include=['datetime', 'object']).columns:\n",
        "        df_cleaned[column] = df_cleaned[column].astype(str)  # Convert datetime objects to strings\n",
        "\n",
        "    # Open the spreadsheet\n",
        "    sheet = client.open(spreadsheet_name)\n",
        "\n",
        "    # Try to get the worksheet by name and delete it\n",
        "    try:\n",
        "        worksheet = sheet.worksheet(worksheet_name)  # Try to access the existing worksheet\n",
        "        sheet.del_worksheet(worksheet)  # Delete the worksheet if it exists\n",
        "        print(f\"Deleted existing worksheet: {worksheet_name}\")\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(f\"Worksheet {worksheet_name} not found, proceeding to create it.\")\n",
        "\n",
        "    # Create a new worksheet with the same name and dimensions of the dataframe\n",
        "    new_worksheet = sheet.add_worksheet(title=worksheet_name, rows=df_cleaned.shape[0] + 1, cols=df_cleaned.shape[1])\n",
        "    print(f\"Created new worksheet: {worksheet_name}\")\n",
        "\n",
        "    # Upload the updated DataFrame to the new worksheet\n",
        "    new_worksheet.update([df_cleaned.columns.values.tolist()] + df_cleaned.values.tolist())\n",
        "    print(f\"Uploaded new data to worksheet: {worksheet_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4rzm-n18OgO"
      },
      "outputs": [],
      "source": [
        "def filter_columns_by_null_percentage(df, percentage_list):\n",
        "    \"\"\"Filters and prints the number of columns with less than specific null value percentages.\"\"\"\n",
        "    # Get column analysis using the analyze_dataframe function\n",
        "    coldata = analyze_columns(df)\n",
        "\n",
        "    for percentage in percentage_list:\n",
        "        # Calculate the threshold for null values based on the percentage\n",
        "        threshold = len(df) * (percentage / 100)\n",
        "\n",
        "        # Filter columns based on the calculated threshold\n",
        "        filtered_cols = coldata[coldata['Null Count'] < threshold]\n",
        "\n",
        "        # Print out the result\n",
        "        print(f\"Number of cols with <{percentage}% null values: \", len(filtered_cols))\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assume 'your_dataframe' is your DataFrame\n",
        "percentage_list = [99, 95, 90, 70, 50, 30, 20, 10, 5, 1]  # Example percentages\n",
        "filter_columns_by_null_percentage(LiveMachines, percentage_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejfAxKjU0HLu"
      },
      "source": [
        "### Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2sboWhvs0IEI"
      },
      "outputs": [],
      "source": [
        "tags_raw = get_tags_from_space(Deployments_SPACEID)\n",
        "\n",
        "for i in range(len(tags_raw['tags'])):\n",
        "  print(tags_raw['tags'][i]['name'], tags_raw['tags'][i]['project_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0louENkb1Sg5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def extract_tag_names(space_id):\n",
        "    \"\"\"\n",
        "    Extracts tag names from a given tags dictionary.\n",
        "\n",
        "    Parameters:\n",
        "        tags_raw (dict): The dictionary containing tag information.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tag names to be used as column names.\n",
        "    \"\"\"\n",
        "    tags_raw = get_tags_from_space(space_id)\n",
        "    tag_names = [tag['name'] for tag in tags_raw.get('tags',{})]\n",
        "    return tag_names\n",
        "\n",
        "extract_tag_names(Deployments_SPACEID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhjAtk-T3Olo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "etsX1v0hD0xd"
      },
      "outputs": [],
      "source": [
        "data = {}\n",
        "for field in range(len(LM_taskdetails['custom_fields'])):\n",
        "  key = LM_taskdetails['custom_fields'][field]['name']\n",
        "  value = LM_taskdetails['custom_fields'][field]['value']\n",
        "  # Skip if key is None or an empty string\n",
        "  if key and key.strip():\n",
        "    data[key] = value\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W1PDkPkwAyIK"
      },
      "outputs": [],
      "source": [
        "[print(LM_taskdetails['custom_fields'][n]['name']) for n in range(len(LM_taskdetails['custom_fields']))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGjw-DsVnkgI"
      },
      "source": [
        "The columns I need to extract:\n",
        "\n",
        "* LocationName = tf['name']\n",
        "* TaskID = tf['id']\n",
        "* Status = tf['status']\n",
        "* Tags = tf['tags']\n",
        "* URL = tf['url']\n",
        "* ListID = tf['list']\n",
        "* ProjectID = tf['project']\n",
        "* FolderID = tf['folder']\n",
        "* SpaceID = tf['space']\n",
        "\n",
        "\n",
        "\n",
        "* CustomField_NAME = tf['custom_fields'][mdata]['name']\n",
        "* CustomField_TYPE = tf['custom_fields'][mdata]['type']\n",
        "* CustomField_VALUE = tf['custom_fields'][mdata]['value']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWyxSccdu_IJ"
      },
      "source": [
        "* TaskID = list_of_tasks[n]['id']\n",
        "* Status = list_of_tasks[n]['status']\n",
        "* AVCustomFields = list_of_tasks[n]['custom_fields']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fNa1007poNs"
      },
      "outputs": [],
      "source": [
        "CF = get_custom_fields_from_list(LiveMachines_LISTID)\n",
        "\n",
        "CF['fields'][8]['type_config']['options'][0]['id']\n",
        "CF['fields'][8]['type_config']['options'][0]['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsWqOXr1kRkH"
      },
      "outputs": [],
      "source": [
        "custom_fields = []\n",
        "\n",
        "for n in range(len(tf['custom_fields'])):\n",
        "  x = [n,tf['custom_fields'][n]['name'], tf['custom_fields'][n]['type']]\n",
        "  custom_fields.append(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbA_kidy_zIs"
      },
      "source": [
        "## Troubleshooting Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHVu9AG8iYft"
      },
      "outputs": [],
      "source": [
        "# Check if API Token works:\n",
        "\n",
        "API_TOKEN = \"pk_88269903_5JXC7PRSK9APTJ4P6FV2KN6RQCLH78CV\"\n",
        "HEADERS = {\n",
        "    \"Authorization\": API_TOKEN,\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Test API Access\n",
        "response = requests.get(\"https://api.clickup.com/api/v2/user\", headers=HEADERS)\n",
        "\n",
        "print(response.status_code)\n",
        "print(response.json())  # Check if authentication works\n",
        "\n",
        "\n",
        "# Check if Task ID works:\n",
        "\n",
        "TASK_ID = \"86b328wdy\"\n",
        "url = f\"https://api.clickup.com/api/v2/task/{TASK_ID}\"\n",
        "response = requests.get(url, headers=HEADERS)\n",
        "\n",
        "print(response.status_code)\n",
        "print(response.json())  # Check the response for errors\n",
        "\n",
        "\n",
        "# Check if List ID works:\n",
        "\n",
        "LIST_ID = \"your_list_id\"\n",
        "url = f\"https://api.clickup.com/api/v2/list/{LiveMachines_LISTID}/task\"\n",
        "response = requests.get(url, headers=HEADERS)\n",
        "print(response.json())  # See if your task is listed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9cTttQgY17o"
      },
      "source": [
        "# Google Sheets\n",
        "\n",
        "\n",
        "Data Sources:\n",
        "*  Partnership Team CRM\n",
        "\n",
        "Figure out all of the datasources and consolidate them in python instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmdOzkkVY49x"
      },
      "source": [
        "## Extract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMoqAmtZrCkL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmsCPeACreuO"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MbFnkrYAq7xL"
      },
      "outputs": [],
      "source": [
        "PartnershipTeamCRM_URL = 'https://docs.google.com/spreadsheets/d/1AfTqoykzjpI6z6bD_XLeWsqe-git4bBKagpg_R7HmDI/export?format=csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP_5hU9ErlGW"
      },
      "outputs": [],
      "source": [
        "# Open the Google Sheet by title or URL\n",
        "sheet = gc.open_by_url(url)\n",
        "\n",
        "# Get the data from the first sheet\n",
        "worksheet = sheet.worksheet(\"Overall\")\n",
        "data = worksheet.get_all_values()\n",
        "\n",
        "# Convert the data to a pandas DataFrame\n",
        "df = pd.DataFrame(data[1:], columns=data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NG8o8SXXruSa"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ejfAxKjU0HLu"
      ],
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1sMTcYj4A5PjqPT1asCYOF2E6E_Nv0XwR",
      "authorship_tag": "ABX9TyM7zhZO57Q4IdBdmxb067dL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}